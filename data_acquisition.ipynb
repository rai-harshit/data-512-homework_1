{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Harshit Rai\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup constants\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "API_LATENCY_ASSUMED = 0.002       \n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<harshit@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      None,         # This value will be replaced by the access type\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     None,         # This value will be replaced by the article title\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"20150701\",   # Start date is 1st July, 2015\n",
    "    \"end\":         \"20230930\"    # End date is 30th September, 2023\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file from which we need the article titles\n",
    "df = pd.read_csv('thank_the_academy.AUG.2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Everything Everywhere All at Once</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Everything_Every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All Quiet on the Western Front (2022 film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/All_Quiet_on_the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Whale (2022 film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Whale_(2022_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top Gun: Maverick</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Top_Gun:_Maverick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black Panther: Wakanda Forever</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Black_Panther:_W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name  \\\n",
       "0           Everything Everywhere All at Once   \n",
       "1  All Quiet on the Western Front (2022 film)   \n",
       "2                       The Whale (2022 film)   \n",
       "3                           Top Gun: Maverick   \n",
       "4              Black Panther: Wakanda Forever   \n",
       "\n",
       "                                                 url  \n",
       "0  https://en.wikipedia.org/wiki/Everything_Every...  \n",
       "1  https://en.wikipedia.org/wiki/All_Quiet_on_the...  \n",
       "2  https://en.wikipedia.org/wiki/The_Whale_(2022_...  \n",
       "3    https://en.wikipedia.org/wiki/Top_Gun:_Maverick  \n",
       "4  https://en.wikipedia.org/wiki/Black_Panther:_W...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if data was read correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order using article title\n",
    "df.sort_values(by=['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>12 Years a Slave (film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/12_Years_a_Slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1917 (2019 film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/1917_(2019_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>20 Feet from Stardom</td>\n",
       "      <td>https://en.wikipedia.org/wiki/20_Feet_from_Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>20,000 Leagues Under the Sea (1954 film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/20,000_Leagues_U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>2001: A Space Odyssey (film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2001:_A_Space_Od...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name  \\\n",
       "137                   12 Years a Slave (film)   \n",
       "53                           1917 (2019 film)   \n",
       "148                      20 Feet from Stardom   \n",
       "819  20,000 Leagues Under the Sea (1954 film)   \n",
       "766              2001: A Space Odyssey (film)   \n",
       "\n",
       "                                                   url  \n",
       "137  https://en.wikipedia.org/wiki/12_Years_a_Slave...  \n",
       "53      https://en.wikipedia.org/wiki/1917_(2019_film)  \n",
       "148  https://en.wikipedia.org/wiki/20_Feet_from_Sta...  \n",
       "819  https://en.wikipedia.org/wiki/20,000_Leagues_U...  \n",
       "766  https://en.wikipedia.org/wiki/2001:_A_Space_Od...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm if the order is correct\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to request pageviews for a given article\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  access_type = None,\n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "\n",
    "    # Access type can be as a parameter to the call or in the request_template\n",
    "    if access_type:\n",
    "        request_template['access'] = access_type\n",
    "    else:\n",
    "        raise Exception(\"Must supply an access type to make a pageviews request.\")\n",
    "    \n",
    "    # Article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['article'] = article_title\n",
    "\n",
    "    if not request_template['article']:\n",
    "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
    "\n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(request_template['article'].replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # Create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # Make the request\n",
    "    try:\n",
    "        # Wait first to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        \n",
    "        # Convert the response to JSON\n",
    "        json_response = response.json()\n",
    "\n",
    "        if 'items' in json_response:\n",
    "            pageviews_per_month = {}\n",
    "            for item in json_response['items']:\n",
    "                month = item['timestamp'][:6]\n",
    "                pageviews = item['views']\n",
    "                pageviews_per_month[month] = pageviews\n",
    "            return {article_title: pageviews_per_month}\n",
    "        else:\n",
    "            print(f\"No 'items' in the response for {article_title}. Response: {json_response}\")\n",
    "            return {article_title: None}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error Occured for {article_title}: {e}\")\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1359 [00:01<04:55,  4.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1292/1359 [04:18<00:15,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'items' in the response for Victor/Victoria. Response: {'type': 'https://mediawiki.org/wiki/HyperSwitch/errors/not_found#route', 'title': 'Not found.', 'method': 'get', 'uri': '/wikimedia.org/v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/user/Victor/Victoria/monthly/20150701/20230930'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1359/1359 [04:58<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each row of the dataframe and request pageviews for each article for cumulative access type\n",
    "cumulative_views = {}\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    cumulative_views.update(request_pageviews_per_article(\n",
    "                                    article_title = row[0], \n",
    "                                    access_type = 'all-access',\n",
    "                                    endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                    endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                    request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                    headers = REQUEST_HEADERS\n",
    "                                ))\n",
    "\n",
    "# Remove any entries that have None as the value\n",
    "cumulative_views = {k: v for k, v in cumulative_views.items() if v is not None}\n",
    "\n",
    "with open(\"academy_monthly_cumulative_201507-202310.json\", \"w\") as cumulative_file:\n",
    "    json.dump(cumulative_views, cumulative_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1292it [13:04,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'items' in the response for Victor/Victoria. Response: {'type': 'https://mediawiki.org/wiki/HyperSwitch/errors/not_found#route', 'title': 'Not found.', 'method': 'get', 'uri': '/wikimedia.org/v1/metrics/pageviews/per-article/en.wikipedia.org/desktop/user/Victor/Victoria/monthly/20150701/20230930'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1359it [13:46,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each row of the dataframe and request pageviews for each article for desktop access type\n",
    "desktop_views = {}\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    desktop_views.update(request_pageviews_per_article(\n",
    "                                    article_title = row[0], \n",
    "                                    access_type = 'desktop',\n",
    "                                    endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                    endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                    request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                    headers = REQUEST_HEADERS\n",
    "                                ))\n",
    "    \n",
    "# Remove any entries that have None as the value\n",
    "desktop_views = {k: v for k, v in desktop_views.items() if v is not None}\n",
    "\n",
    "with open(\"academy_monthly_desktop_201507-202310.json\", \"w\") as desktop_file:\n",
    "    json.dump(desktop_views, desktop_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1292it [13:10,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'items' in the response for Victor/Victoria. Response: {'type': 'https://mediawiki.org/wiki/HyperSwitch/errors/not_found#route', 'title': 'Not found.', 'method': 'get', 'uri': '/wikimedia.org/v1/metrics/pageviews/per-article/en.wikipedia.org/mobile-app/user/Victor/Victoria/monthly/20150701/20230930'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1359it [13:52,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each row of the dataframe and request pageviews for each article for mobile-app access type\n",
    "mobile_app_views = {}\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    mobile_app_views.update(request_pageviews_per_article(\n",
    "                                    article_title = row[0], \n",
    "                                    access_type = 'mobile-app',\n",
    "                                    endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                    endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                    request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                    headers = REQUEST_HEADERS\n",
    "                                ))\n",
    "\n",
    "# Remove any entries that have None as the value\n",
    "mobile_app_views = {k: v for k, v in mobile_app_views.items() if v is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1292it [13:21,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'items' in the response for Victor/Victoria. Response: {'type': 'https://mediawiki.org/wiki/HyperSwitch/errors/not_found#route', 'title': 'Not found.', 'method': 'get', 'uri': '/wikimedia.org/v1/metrics/pageviews/per-article/en.wikipedia.org/mobile-web/user/Victor/Victoria/monthly/20150701/20230930'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1359it [14:04,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each row of the dataframe and request pageviews for each article for mobile-web access type\n",
    "mobile_web_views = {}\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    mobile_web_views.update(request_pageviews_per_article(\n",
    "                                    article_title = row[0], \n",
    "                                    access_type = 'mobile-web',\n",
    "                                    endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                    endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                    request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                    headers = REQUEST_HEADERS\n",
    "                                ))\n",
    "\n",
    "# Remove any entries that have None as the value\n",
    "mobile_web_views = {k: v for k, v in mobile_web_views.items() if v is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add view counts from mobile-app and mobile-web together\n",
    "for key in mobile_web_views:\n",
    "        # Check if the key exists in the second dictionary\n",
    "        if key in mobile_app_views:\n",
    "            # Now, for each month-key inside the movie's dictionary:\n",
    "            for sub_key in mobile_web_views[key]:\n",
    "                # Add the values of the matching keys together\n",
    "                mobile_web_views[key][sub_key] += mobile_app_views[key].get(sub_key, 0)\n",
    "\n",
    "with open(\"academy_monthly_mobile_201507-202310.json\", \"w\") as mobile_file:\n",
    "    json.dump(mobile_web_views, mobile_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
